name: Deploy Flaskapp USER to EKS Cluster

#on: [ workflow_dispatch ] 
on:
  push:
    branches: [ "testing_on_aws", "main" ]

env:
  AWS_REGION: ${{secrets.AWS_REGION}}
  ECR_REPOSITORY: flaskapp1-demo-shop
  ECR_REGISTRY: ${{secrets.AWS_ACCOUNT_ID}}.dkr.ecr.${{secrets.AWS_REGION}}.amazonaws.com"
  EKS_CLUSTER_NAME: webapps-demo
  CONTAINER_NAME: flaskapp1-user
  EKS_APP_NAME: flaskapp1-user
  EKS_SERVICE: flaskapp1
  EKS_SERVICE_ACCOUNT: flaskapp-sa1
  EKS_NAMESPACE: flaskapp
  EKS_APP_USER_NAME: "flaskapp1-user"
  EKS_DEPLOYMENT_NAME: flaskapp1-user
  APP_DIR: dockerized/USER
  MANIFESTS_DIR: aws/eks/deploy/manifest/flaskapp1
  AWS_ACCOUNT_ID: ${{secrets.AWS_ACCOUNT_ID}}
  AWS_APP_USER_PORT: 50443
  GITHUB_SHA: ${{ github.sha }}
  IMAGE_NAME: ${{secrets.AWS_ACCOUNT_ID}}.dkr.ecr.${{secrets.AWS_REGION}}.amazonaws.com/flaskapp1-demo-shop:${{github.sha}}-user
  FLASK_USER_IMAGE_NAME: ${{secrets.AWS_ACCOUNT_ID}}.dkr.ecr.${{secrets.AWS_REGION}}.amazonaws.com/flaskapp1-demo-shop:${{github.sha}}-user
jobs:
  Build:
    runs-on: ubuntu-latest
    outputs:
      image: ${{ steps.build-image.outputs.image }}

    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          mask-aws-account-id: no

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1

      - name: Build, tag, and push image to Amazon ECR
        id: build-image
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ env.GITHUB_SHA }}
        working-directory: ${{env.APP_DIR}}
        run: |
          imagename=${{env.FLASK_USER_IMAGE_NAME}}
          echo "Build and push $imagename"
          docker build -t $imagename .
          docker push $imagename
          echo "image=$imagename" >> $GITHUB_OUTPUT
  
  #Image scan with Aqua Trivy     
  Scan:
    needs: [Build]
    runs-on: ubuntu-latest
    steps:
      - uses: aws-actions/configure-aws-credentials@v1
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
          mask-aws-account-id: no    
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: '${{env.FLASK_USER_IMAGE_NAME}}'
          format: 'table'
          #exit-code: '1' #By pass and continue if vulnerabilities found (Not recommended, testing complete devSecOps pipeline)
          ignore-unfixed: true
          vuln-type: 'os,library'
          severity: 'CRITICAL,HIGH'  

  Deploy:
    runs-on: ubuntu-latest
    environment: Staging
    needs: [Scan]

    steps:
      - name: Checkout
        uses: actions/checkout@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v1
        env:
          KUBE_CONFIG_DATA: ${{ secrets.KUBE_CONFIG_DATA }}
          IMAGE_NAME: ${{env.FLASK_USER_IMAGE_NAME}}
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}
      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v1
      
      #Configure kubectl
      - name: Configure kubectl
        run: |
          echo ${{ secrets.KUBE_CONFIG_DATA }} | base64 --decode > kubeconfig.yaml
          mkdir -p ~/.kube
          mv kubeconfig.yaml ~/.kube/config
      #Deploy application  
      - name: Deploy
        working-directory: ${{env.MANIFESTS_DIR}}
        run: |-
          envsubst < flaskapp1.yaml | kubectl apply -f -
          envsubst < Service_User.yaml | kubectl apply -f -
          envsubst < Deployment_User.yaml | kubectl apply -f -
          kubectl get pods -o wide -n ${{env.EKS_NAMESPACE}}
          kubectl get svc -o wide -n ${{env.EKS_NAMESPACE}}
          sleep 90
      
      #Check application is running, test REST endpoints
      - name: Verify Coontainer Application
        run: |-
          kubectl rollout status deploy ${{env.EKS_APP_NAME}} -n ${{env.EKS_NAMESPACE}}
          curl -Lk https://`kubectl get svc -n ${{env.EKS_NAMESPACE}} | grep ${{env.EKS_APP_USER_NAME}} | awk '{print $4}'`:${{env.AWS_APP_USER_PORT}}/login
          curl -Lk https://`kubectl get svc -n ${{env.EKS_NAMESPACE}} | grep ${{env.EKS_APP_USER_NAME}} | awk '{print $4}'`:${{env.AWS_APP_USER_PORT}}/restore
          curl -Lk https://`kubectl get svc -n ${{env.EKS_NAMESPACE}} | grep ${{env.EKS_APP_USER_NAME}} | awk '{print $4}'`:${{env.AWS_APP_USER_PORT}}/status
          curl -Lk https://`kubectl get svc -n ${{env.EKS_NAMESPACE}} | grep ${{env.EKS_APP_USER_NAME}} | awk '{print $4}'`:${{env.AWS_APP_USER_PORT}}/attachdb

      #Notify Slack
      - name: Notify Slack
        uses: ravsamhq/notify-slack-action@v2
        if: always()
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_CI_HOOK }}
        with:
          status: ${{ job.status }}
          notification_title: "{workflow}:{job} has {status_message}"
          message_format: "{emoji} *{workflow}:{job}* {status_message} in <{repo_url}|{repo}>"
          footer: "Logs <{run_url}|{job}>"
          notify_when: "failure,success"       

  # zap_scan:
  #   runs-on: ubuntu-latest
  #   name: zap_scan
  #   needs: [Deploy]
  #   steps:
  #     - name: Checkout
  #       uses: actions/checkout@v2
  #       with:
  #         ref: master
  #     - name: ZAP Scan
  #       uses: zaproxy/action-baseline@v0.6.1
  #       with:
  #         token: ${{ secrets.GITHUB_TOKEN }}
  #         docker_name: 'owasp/zap2docker-stable'
  #         target: "https://20.124.47.56:25443/backup/status"
  #         # target: ${{env.CREATE_EP}}
  #         rules_file_name: '.zap/rules.tsv'
  #         cmd_options: '-a'   